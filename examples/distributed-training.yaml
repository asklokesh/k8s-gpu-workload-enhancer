# Example: Distributed LLM Training with KGWE
# This example demonstrates topology-aware scheduling for a distributed
# training job using PyTorch FSDP on 8 H100 GPUs.
#
# KGWE will automatically:
# - Place all GPUs on nodes with NVLink connectivity
# - Optimize for maximum inter-GPU bandwidth
# - Track costs and attribute them to the team

apiVersion: kgwe.nvidia.io/v1
kind: GPUWorkload
metadata:
  name: llm-fsdp-training
  namespace: ml-research
  labels:
    team: foundation-models
    project: llama-finetune
spec:
  # GPU Requirements
  gpuRequirements:
    count: 8
    minMemoryGB: 80        # H100 80GB
    gpuModel: H100
    topology:
      preference: NVLinkOptimal
      minBandwidthGBps: 600   # NVSwitch bandwidth

  # Workload Classification
  workloadType: Training
  framework: PyTorch

  # Distributed Configuration
  distributedConfig:
    strategy: FSDP          # Fully Sharded Data Parallel
    worldSize: 8
    backend: NCCL

  # Priority (higher = more important)
  priority: 100

  # Can be preempted for higher priority jobs
  preemptible: false

  # Pod Template
  podTemplate:
    spec:
      containers:
        - name: trainer
          image: nvcr.io/nvidia/pytorch:24.01-py3
          command:
            - torchrun
            - --nproc_per_node=8
            - --master_addr=$(MASTER_ADDR)
            - --master_port=29500
            - /workspace/train.py
            - --model=llama-7b
            - --batch-size=32
            - --gradient-accumulation=4
            - --learning-rate=1e-5
            - --epochs=3
          env:
            - name: NCCL_DEBUG
              value: "INFO"
            - name: NCCL_IB_DISABLE
              value: "0"
            - name: CUDA_DEVICE_MAX_CONNECTIONS
              value: "1"
          resources:
            limits:
              nvidia.com/gpu: 8
              memory: 512Gi
              cpu: 64
            requests:
              nvidia.com/gpu: 8
              memory: 256Gi
              cpu: 32
          volumeMounts:
            - name: data
              mountPath: /data
            - name: checkpoints
              mountPath: /checkpoints
            - name: shm
              mountPath: /dev/shm
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: training-data
        - name: checkpoints
          persistentVolumeClaim:
            claimName: model-checkpoints
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi
      restartPolicy: OnFailure

---
# Example: MIG-based Inference Workload
# Uses a small MIG slice for cost-efficient inference

apiVersion: kgwe.nvidia.io/v1
kind: GPUWorkload
metadata:
  name: llm-inference
  namespace: ml-production
  labels:
    team: inference-platform
    project: chatbot-api
spec:
  gpuRequirements:
    count: 1
    mig:
      profile: 2g.20gb     # Small MIG slice
      count: 1

  workloadType: Inference
  framework: Triton

  priority: 50
  preemptible: true        # Can be preempted

  podTemplate:
    spec:
      containers:
        - name: inference
          image: nvcr.io/nvidia/tritonserver:24.01-py3
          args:
            - tritonserver
            - --model-repository=/models
            - --http-port=8000
            - --grpc-port=8001
            - --metrics-port=8002
          ports:
            - containerPort: 8000
              name: http
            - containerPort: 8001
              name: grpc
            - containerPort: 8002
              name: metrics
          resources:
            limits:
              nvidia.com/mig-2g.20gb: 1
              memory: 32Gi
              cpu: 8
          volumeMounts:
            - name: models
              mountPath: /models
          livenessProbe:
            httpGet:
              path: /v2/health/live
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /v2/health/ready
              port: 8000
            initialDelaySeconds: 30
            periodSeconds: 5
      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: model-repository

---
# Example: MIG Strategy for Production Cluster
# Configures MIG partitioning policy for a node pool

apiVersion: kgwe.nvidia.io/v1
kind: MIGStrategy
metadata:
  name: inference-optimized
spec:
  nodeSelector:
    gpu.nvidia.com/class: H100
    node-pool: inference

  gpuSelector:
    model: H100
    migCapable: true

  # Target 70% small instances, 20% medium, 10% large
  profileDistribution:
    "1g.10gb": 0.70
    "2g.20gb": 0.20
    "3g.40gb": 0.10

  # Allow dynamic reconfiguration
  allowDynamicReconfig: true
  rebalanceInterval: "5m"
  minUtilizationThreshold: 0.3

  priority: 10

---
# Example: GPU Budget for ML Team
# Sets spending limits with alerts

apiVersion: kgwe.nvidia.io/v1
kind: GPUBudget
metadata:
  name: ml-research-budget
  namespace: ml-research
spec:
  limit: "10000"           # $10,000 per month
  currency: USD
  period: Monthly
  scope: Namespace

  alertsEnabled: true
  alertThresholds:
    - 0.5                  # Alert at 50%
    - 0.75                 # Alert at 75%
    - 0.9                  # Alert at 90%
    - 1.0                  # Alert at 100%

  enforcementPolicy: Alert  # Alert but don't block
